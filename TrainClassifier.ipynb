{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic classification of stadsarchief images\n",
    "Network based on: https://towardsdatascience.com/build-your-own-convolution-neural-network-in-5-mins-4217c2cf964f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "from data import load_data\n",
    "import keras.backend as K\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first few files: ['examples/0-src/200x200/SA00086044_00002.jpg', 'examples/0-src/200x200/SA00086044_00003.jpg', 'examples/0-src/200x200/SA00086044_00004.jpg', 'examples/0-src/200x200/SA00086083_00001.jpg', 'examples/0-src/200x200/SA00086083_00002.jpg']\n",
      "first labels   : ['examples/0-src/labels/SA00086044_00002.yaml', 'examples/0-src/labels/SA00086044_00003.yaml', 'examples/0-src/labels/SA00086044_00004.yaml', 'examples/0-src/labels/SA00086083_00001.yaml', 'examples/0-src/labels/SA00086083_00002.yaml']\n",
      "(65, 200, 200, 3)\n",
      "(65, 200, 200, 3)\n",
      "(65,)\n",
      "{'bouwaanvraag', 'map', 'bouwverordening', 'calculation', 'blueprint', 'other', 'kennisgeving', 'letter'}\n",
      "{'other', 'blueprint'}\n"
     ]
    }
   ],
   "source": [
    "IMG_DIR = 'examples/0-src/200x200/';\n",
    "LABEL_DIR = 'examples/0-src/labels/';  \n",
    "    \n",
    "[X, Ystr] = load_data(IMG_DIR, LABEL_DIR)\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    X_shuffled = X[indices]\n",
    "    Y_shuffled = Y[indices]\n",
    "    return (X_shuffled, Y_shuffled)\n",
    "print(X.shape)\n",
    "\n",
    "X_shuffled, Y_shuffled = shuffle(X, Ystr)\n",
    "print(X_shuffled.shape)\n",
    "print(Y_shuffled.shape)\n",
    "X = X_shuffled\n",
    "Ystr = Y_shuffled\n",
    "\n",
    "def simplify_Y(Y):\n",
    "    return np.array([y if y == 'blueprint' else 'other' for y in Y])\n",
    "\n",
    "# print(Ystr)\n",
    "print(set(Ystr))\n",
    "Ystr = simplify_Y(Ystr)\n",
    "# print(Ystr)\n",
    "print(set(Ystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 200, 200, 3)\n",
      "(65,)\n",
      "number of classes: 2\n",
      "types encountered: ['other', 'blueprint']\n",
      "['other' 'other' 'other']\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Ystr.shape)\n",
    "\n",
    "types = list(set(Ystr))\n",
    "num_classes = len(types)\n",
    "print(f\"number of classes: {num_classes}\")\n",
    "print(f\"types encountered: {types}\")\n",
    "\n",
    "def convert_to_index(Y, types):\n",
    "    return np.array([types.index(y) for y in Y])\n",
    "\n",
    "print(Ystr[:3])\n",
    "Yind = convert_to_index(Ystr, types)\n",
    "print(Yind[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_class(Y, class_idx):\n",
    "    print(Y)\n",
    "    Y[:] = class_idx\n",
    "    print(Y)\n",
    "    return Y\n",
    "# YInd = set_class(Yind, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = keras.utils.to_categorical(Yind, num_classes)  # convert class vectors to binary class matrices\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "split data shapes:\n",
      "(40, 200, 200, 3)\n",
      "(40, 2)\n",
      "(25, 200, 200, 3)\n",
      "(25, 2)\n"
     ]
    }
   ],
   "source": [
    "def split_data(X, Y, size):\n",
    "    X_train = X[:N]\n",
    "    Y_train = Y[:N]\n",
    "    X_test = X[N:]\n",
    "    Y_test = Y[N:]\n",
    "    return (X_train, Y_train), (X_test, Y_test)\n",
    "\n",
    "\n",
    "img_dim = (200, 200, 3);\n",
    "N = 40\n",
    "(X_train, Y_train), (X_test, Y_test) = split_data(X, Y, N)\n",
    "print(\"\")\n",
    "print(\"split data shapes:\")\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      "{0: 26, 1: 14}\n",
      "test: \n",
      "{0: 19, 1: 6}\n"
     ]
    }
   ],
   "source": [
    "def list_stats(X, Y):\n",
    "    Yidx = np.argmax(Y, axis=1)\n",
    "    unique, counts = np.unique(Yidx, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "print(\"train: \")\n",
    "list_stats(X_train, Y_train)\n",
    "print(\"test: \")\n",
    "list_stats(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split data shapes:\n",
      "(40, 200, 200, 3)\n",
      "(40, 2)\n",
      "(25, 200, 200, 3)\n",
      "(25, 2)\n",
      "split data shapes:\n",
      "(40, 20, 20, 3)\n",
      "(40, 2)\n",
      "(25, 20, 20, 3)\n",
      "(25, 2)\n"
     ]
    }
   ],
   "source": [
    "resize = 20\n",
    "print(\"split data shapes:\")\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "X_train = X_train[:, :resize, :resize, :]\n",
    "X_test = X_test[:, :resize, :resize, :]\n",
    "print(\"split data shapes:\")\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "img_dim = (resize, resize, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 18, 18, 2)         56        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 18, 18, 2)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 648)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                10384     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 10,474\n",
      "Trainable params: 10,474\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# def build_model(num_classes, img_dim):\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "#                      activation='relu',\n",
    "#                      input_shape=img_dim))\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(128, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "#     return model\n",
    "\n",
    "def build_model(num_classes, img_dim):\n",
    "    print(img_dim)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(2, kernel_size=(3, 3),\n",
    "                     activation='linear',\n",
    "                     input_shape=img_dim,\n",
    "                     kernel_initializer = RandomUniform()\n",
    "                    ))\n",
    "    model.add(LeakyReLU())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())    \n",
    "#     model.add(Flatten(input_shape=img_dim))\n",
    "    model.add(Dense(16, activation='linear', kernel_initializer = RandomUniform()))\n",
    "    model.add(LeakyReLU())\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "model = build_model(num_classes, img_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40 samples, validate on 25 samples\n",
      "Epoch 1/200\n",
      "40/40 [==============================] - 1s 13ms/step - loss: 4.0637 - acc: 0.3750 - mean_pred: 0.5000 - val_loss: 1.3257 - val_acc: 0.4000 - val_mean_pred: 0.5000\n",
      "Epoch 2/200\n",
      "40/40 [==============================] - 0s 157us/step - loss: 1.9778 - acc: 0.3000 - mean_pred: 0.5000 - val_loss: 0.8469 - val_acc: 0.8000 - val_mean_pred: 0.5000\n",
      "Epoch 3/200\n",
      "40/40 [==============================] - 0s 149us/step - loss: 1.6730 - acc: 0.5750 - mean_pred: 0.5000 - val_loss: 0.9656 - val_acc: 0.8000 - val_mean_pred: 0.5000\n",
      "Epoch 4/200\n",
      "40/40 [==============================] - 0s 151us/step - loss: 1.8423 - acc: 0.5750 - mean_pred: 0.5000 - val_loss: 0.9666 - val_acc: 0.8000 - val_mean_pred: 0.5000\n",
      "Epoch 5/200\n",
      "40/40 [==============================] - 0s 144us/step - loss: 1.8126 - acc: 0.6250 - mean_pred: 0.5000 - val_loss: 0.8775 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 6/200\n",
      "40/40 [==============================] - 0s 136us/step - loss: 1.6300 - acc: 0.6500 - mean_pred: 0.5000 - val_loss: 0.7456 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 7/200\n",
      "40/40 [==============================] - 0s 132us/step - loss: 1.3676 - acc: 0.6500 - mean_pred: 0.5000 - val_loss: 0.5980 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 8/200\n",
      "40/40 [==============================] - 0s 126us/step - loss: 1.0749 - acc: 0.6750 - mean_pred: 0.5000 - val_loss: 0.4613 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 9/200\n",
      "40/40 [==============================] - 0s 144us/step - loss: 0.7995 - acc: 0.7250 - mean_pred: 0.5000 - val_loss: 0.3989 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 10/200\n",
      "40/40 [==============================] - 0s 144us/step - loss: 0.6159 - acc: 0.7500 - mean_pred: 0.5000 - val_loss: 0.4736 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 11/200\n",
      "40/40 [==============================] - 0s 136us/step - loss: 0.6094 - acc: 0.6250 - mean_pred: 0.5000 - val_loss: 0.6181 - val_acc: 0.6800 - val_mean_pred: 0.5000\n",
      "Epoch 12/200\n",
      "40/40 [==============================] - 0s 138us/step - loss: 0.7024 - acc: 0.6000 - mean_pred: 0.5000 - val_loss: 0.6586 - val_acc: 0.6400 - val_mean_pred: 0.5000\n",
      "Epoch 13/200\n",
      "40/40 [==============================] - 0s 141us/step - loss: 0.7162 - acc: 0.6000 - mean_pred: 0.5000 - val_loss: 0.5847 - val_acc: 0.6800 - val_mean_pred: 0.5000\n",
      "Epoch 14/200\n",
      "40/40 [==============================] - 0s 134us/step - loss: 0.6340 - acc: 0.6500 - mean_pred: 0.5000 - val_loss: 0.4636 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 15/200\n",
      "40/40 [==============================] - 0s 131us/step - loss: 0.5222 - acc: 0.7750 - mean_pred: 0.5000 - val_loss: 0.3474 - val_acc: 0.9200 - val_mean_pred: 0.5000\n",
      "Epoch 16/200\n",
      "40/40 [==============================] - 0s 152us/step - loss: 0.4311 - acc: 0.8750 - mean_pred: 0.5000 - val_loss: 0.2719 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 17/200\n",
      "40/40 [==============================] - 0s 138us/step - loss: 0.3839 - acc: 0.7750 - mean_pred: 0.5000 - val_loss: 0.2434 - val_acc: 0.8800 - val_mean_pred: 0.5000\n",
      "Epoch 18/200\n",
      "40/40 [==============================] - 0s 150us/step - loss: 0.3780 - acc: 0.7250 - mean_pred: 0.5000 - val_loss: 0.2447 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 19/200\n",
      "40/40 [==============================] - 0s 140us/step - loss: 0.4002 - acc: 0.7500 - mean_pred: 0.5000 - val_loss: 0.2596 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 20/200\n",
      "40/40 [==============================] - 0s 125us/step - loss: 0.4314 - acc: 0.7500 - mean_pred: 0.5000 - val_loss: 0.2588 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 21/200\n",
      "40/40 [==============================] - 0s 148us/step - loss: 0.4306 - acc: 0.7500 - mean_pred: 0.5000 - val_loss: 0.2385 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 22/200\n",
      "40/40 [==============================] - 0s 128us/step - loss: 0.3901 - acc: 0.7500 - mean_pred: 0.5000 - val_loss: 0.2129 - val_acc: 0.8400 - val_mean_pred: 0.5000\n",
      "Epoch 23/200\n",
      "40/40 [==============================] - 0s 132us/step - loss: 0.3390 - acc: 0.7250 - mean_pred: 0.5000 - val_loss: 0.1973 - val_acc: 0.8800 - val_mean_pred: 0.5000\n",
      "Epoch 24/200\n",
      "40/40 [==============================] - 0s 152us/step - loss: 0.3031 - acc: 0.7750 - mean_pred: 0.5000 - val_loss: 0.1934 - val_acc: 0.9600 - val_mean_pred: 0.5000\n",
      "Epoch 25/200\n",
      "40/40 [==============================] - 0s 119us/step - loss: 0.2868 - acc: 0.8500 - mean_pred: 0.5000 - val_loss: 0.2029 - val_acc: 0.9600 - val_mean_pred: 0.5000\n",
      "Epoch 26/200\n",
      "40/40 [==============================] - 0s 167us/step - loss: 0.2877 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.2169 - val_acc: 0.9600 - val_mean_pred: 0.5000\n",
      "Epoch 27/200\n",
      "40/40 [==============================] - 0s 143us/step - loss: 0.2960 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.2246 - val_acc: 0.9600 - val_mean_pred: 0.5000\n",
      "Epoch 28/200\n",
      "40/40 [==============================] - 0s 132us/step - loss: 0.3019 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.2183 - val_acc: 0.9600 - val_mean_pred: 0.5000\n",
      "Epoch 29/200\n",
      "40/40 [==============================] - 0s 154us/step - loss: 0.2962 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1996 - val_acc: 0.9600 - val_mean_pred: 0.5000\n",
      "Epoch 30/200\n",
      "40/40 [==============================] - 0s 131us/step - loss: 0.2788 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1765 - val_acc: 0.9600 - val_mean_pred: 0.5000\n",
      "Epoch 31/200\n",
      "40/40 [==============================] - 0s 153us/step - loss: 0.2577 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1579 - val_acc: 0.9600 - val_mean_pred: 0.5000\n",
      "Epoch 32/200\n",
      "40/40 [==============================] - 0s 149us/step - loss: 0.2398 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1451 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 33/200\n",
      "40/40 [==============================] - 0s 129us/step - loss: 0.2291 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1384 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 34/200\n",
      "40/40 [==============================] - 0s 167us/step - loss: 0.2250 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1349 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 35/200\n",
      "40/40 [==============================] - 0s 147us/step - loss: 0.2265 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1336 - val_acc: 0.9600 - val_mean_pred: 0.5000\n",
      "Epoch 36/200\n",
      "40/40 [==============================] - 0s 144us/step - loss: 0.2284 - acc: 0.8750 - mean_pred: 0.5000 - val_loss: 0.1310 - val_acc: 0.9600 - val_mean_pred: 0.5000\n",
      "Epoch 37/200\n",
      "40/40 [==============================] - 0s 170us/step - loss: 0.2267 - acc: 0.8750 - mean_pred: 0.5000 - val_loss: 0.1253 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 38/200\n",
      "40/40 [==============================] - 0s 154us/step - loss: 0.2202 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1179 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 39/200\n",
      "40/40 [==============================] - 0s 145us/step - loss: 0.2110 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1115 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 40/200\n",
      "40/40 [==============================] - 0s 149us/step - loss: 0.2031 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1077 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 41/200\n",
      "40/40 [==============================] - 0s 154us/step - loss: 0.1989 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1061 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 42/200\n",
      "40/40 [==============================] - 0s 141us/step - loss: 0.1981 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1055 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 43/200\n",
      "40/40 [==============================] - 0s 159us/step - loss: 0.1990 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1047 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 44/200\n",
      "40/40 [==============================] - 0s 180us/step - loss: 0.1999 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.1028 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 45/200\n",
      "40/40 [==============================] - 0s 143us/step - loss: 0.1990 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.0996 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 46/200\n",
      "40/40 [==============================] - 0s 152us/step - loss: 0.1962 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.0956 - val_acc: 1.0000 - val_mean_pred: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/200\n",
      "40/40 [==============================] - 0s 138us/step - loss: 0.1925 - acc: 0.9000 - mean_pred: 0.5000 - val_loss: 0.0924 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 48/200\n",
      "40/40 [==============================] - 0s 164us/step - loss: 0.1890 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0905 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 49/200\n",
      "40/40 [==============================] - 0s 187us/step - loss: 0.1874 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0901 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 50/200\n",
      "40/40 [==============================] - 0s 151us/step - loss: 0.1869 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0904 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 51/200\n",
      "40/40 [==============================] - 0s 179us/step - loss: 0.1869 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0905 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 52/200\n",
      "40/40 [==============================] - 0s 156us/step - loss: 0.1869 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0896 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 53/200\n",
      "40/40 [==============================] - 0s 142us/step - loss: 0.1857 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0875 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 54/200\n",
      "40/40 [==============================] - 0s 140us/step - loss: 0.1833 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0852 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 55/200\n",
      "40/40 [==============================] - 0s 124us/step - loss: 0.1809 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0832 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 56/200\n",
      "40/40 [==============================] - 0s 133us/step - loss: 0.1794 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0818 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 57/200\n",
      "40/40 [==============================] - 0s 151us/step - loss: 0.1787 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0808 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 58/200\n",
      "40/40 [==============================] - 0s 158us/step - loss: 0.1782 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0800 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 59/200\n",
      "40/40 [==============================] - 0s 146us/step - loss: 0.1774 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0792 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 60/200\n",
      "40/40 [==============================] - 0s 135us/step - loss: 0.1762 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0785 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 61/200\n",
      "40/40 [==============================] - 0s 138us/step - loss: 0.1745 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0781 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 62/200\n",
      "40/40 [==============================] - 0s 136us/step - loss: 0.1729 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0783 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 63/200\n",
      "40/40 [==============================] - 0s 125us/step - loss: 0.1717 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0788 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 64/200\n",
      "40/40 [==============================] - 0s 133us/step - loss: 0.1709 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0790 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 65/200\n",
      "40/40 [==============================] - 0s 146us/step - loss: 0.1702 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0785 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 66/200\n",
      "40/40 [==============================] - 0s 147us/step - loss: 0.1692 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0774 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 67/200\n",
      "40/40 [==============================] - 0s 139us/step - loss: 0.1678 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0761 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 68/200\n",
      "40/40 [==============================] - 0s 137us/step - loss: 0.1666 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0749 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 69/200\n",
      "40/40 [==============================] - 0s 143us/step - loss: 0.1655 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0739 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 70/200\n",
      "40/40 [==============================] - 0s 153us/step - loss: 0.1647 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0733 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 71/200\n",
      "40/40 [==============================] - 0s 133us/step - loss: 0.1639 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0730 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 72/200\n",
      "40/40 [==============================] - 0s 136us/step - loss: 0.1629 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0727 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 73/200\n",
      "40/40 [==============================] - 0s 141us/step - loss: 0.1618 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0728 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 74/200\n",
      "40/40 [==============================] - 0s 166us/step - loss: 0.1607 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0730 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 75/200\n",
      "40/40 [==============================] - 0s 142us/step - loss: 0.1596 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0733 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 76/200\n",
      "40/40 [==============================] - 0s 144us/step - loss: 0.1587 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0736 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 77/200\n",
      "40/40 [==============================] - 0s 142us/step - loss: 0.1579 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0734 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 78/200\n",
      "40/40 [==============================] - 0s 132us/step - loss: 0.1570 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0728 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 79/200\n",
      "40/40 [==============================] - 0s 135us/step - loss: 0.1560 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0719 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 80/200\n",
      "40/40 [==============================] - 0s 142us/step - loss: 0.1549 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0709 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 81/200\n",
      "40/40 [==============================] - 0s 151us/step - loss: 0.1540 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0703 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 82/200\n",
      "40/40 [==============================] - 0s 134us/step - loss: 0.1531 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0698 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 83/200\n",
      "40/40 [==============================] - 0s 153us/step - loss: 0.1522 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0696 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 84/200\n",
      "40/40 [==============================] - 0s 155us/step - loss: 0.1513 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0696 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 85/200\n",
      "40/40 [==============================] - 0s 137us/step - loss: 0.1503 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0698 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 86/200\n",
      "40/40 [==============================] - 0s 149us/step - loss: 0.1494 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0700 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 87/200\n",
      "40/40 [==============================] - 0s 132us/step - loss: 0.1485 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0701 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 88/200\n",
      "40/40 [==============================] - 0s 155us/step - loss: 0.1476 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0700 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 89/200\n",
      "40/40 [==============================] - 0s 132us/step - loss: 0.1467 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0696 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 90/200\n",
      "40/40 [==============================] - 0s 154us/step - loss: 0.1458 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0690 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 91/200\n",
      "40/40 [==============================] - 0s 145us/step - loss: 0.1449 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0685 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 92/200\n",
      "40/40 [==============================] - 0s 138us/step - loss: 0.1440 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0681 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 93/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 139us/step - loss: 0.1431 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0678 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 94/200\n",
      "40/40 [==============================] - 0s 139us/step - loss: 0.1423 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0676 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 95/200\n",
      "40/40 [==============================] - 0s 143us/step - loss: 0.1414 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0677 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 96/200\n",
      "40/40 [==============================] - 0s 162us/step - loss: 0.1405 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0677 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 97/200\n",
      "40/40 [==============================] - 0s 146us/step - loss: 0.1396 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0678 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 98/200\n",
      "40/40 [==============================] - 0s 130us/step - loss: 0.1388 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0677 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 99/200\n",
      "40/40 [==============================] - 0s 130us/step - loss: 0.1380 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0675 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 100/200\n",
      "40/40 [==============================] - 0s 138us/step - loss: 0.1371 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0671 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 101/200\n",
      "40/40 [==============================] - 0s 146us/step - loss: 0.1362 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0667 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 102/200\n",
      "40/40 [==============================] - 0s 131us/step - loss: 0.1354 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0664 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 103/200\n",
      "40/40 [==============================] - 0s 139us/step - loss: 0.1346 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0661 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 104/200\n",
      "40/40 [==============================] - 0s 138us/step - loss: 0.1338 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0660 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 105/200\n",
      "40/40 [==============================] - 0s 137us/step - loss: 0.1329 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0660 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 106/200\n",
      "40/40 [==============================] - 0s 139us/step - loss: 0.1321 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0659 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 107/200\n",
      "40/40 [==============================] - 0s 142us/step - loss: 0.1313 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0659 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 108/200\n",
      "40/40 [==============================] - 0s 148us/step - loss: 0.1305 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0658 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 109/200\n",
      "40/40 [==============================] - 0s 127us/step - loss: 0.1297 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0657 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 110/200\n",
      "40/40 [==============================] - 0s 150us/step - loss: 0.1289 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0654 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 111/200\n",
      "40/40 [==============================] - 0s 137us/step - loss: 0.1281 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0651 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 112/200\n",
      "40/40 [==============================] - 0s 145us/step - loss: 0.1273 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0647 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 113/200\n",
      "40/40 [==============================] - 0s 145us/step - loss: 0.1265 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0645 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 114/200\n",
      "40/40 [==============================] - 0s 125us/step - loss: 0.1258 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0643 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 115/200\n",
      "40/40 [==============================] - 0s 144us/step - loss: 0.1250 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0641 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 116/200\n",
      "40/40 [==============================] - 0s 147us/step - loss: 0.1242 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0641 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 117/200\n",
      "40/40 [==============================] - 0s 141us/step - loss: 0.1235 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0640 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 118/200\n",
      "40/40 [==============================] - 0s 135us/step - loss: 0.1227 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0639 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 119/200\n",
      "40/40 [==============================] - 0s 133us/step - loss: 0.1220 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0637 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 120/200\n",
      "40/40 [==============================] - 0s 144us/step - loss: 0.1212 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0635 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 121/200\n",
      "40/40 [==============================] - 0s 142us/step - loss: 0.1205 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0632 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 122/200\n",
      "40/40 [==============================] - 0s 141us/step - loss: 0.1198 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0630 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 123/200\n",
      "40/40 [==============================] - 0s 150us/step - loss: 0.1190 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0627 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 124/200\n",
      "40/40 [==============================] - 0s 138us/step - loss: 0.1183 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0626 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 125/200\n",
      "40/40 [==============================] - 0s 148us/step - loss: 0.1176 - acc: 0.9250 - mean_pred: 0.5000 - val_loss: 0.0626 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 126/200\n",
      "40/40 [==============================] - 0s 141us/step - loss: 0.1168 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0627 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 127/200\n",
      "40/40 [==============================] - 0s 124us/step - loss: 0.1160 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0629 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 128/200\n",
      "40/40 [==============================] - 0s 126us/step - loss: 0.1151 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0631 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 129/200\n",
      "40/40 [==============================] - 0s 157us/step - loss: 0.1143 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0631 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 130/200\n",
      "40/40 [==============================] - 0s 152us/step - loss: 0.1134 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0629 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 131/200\n",
      "40/40 [==============================] - 0s 135us/step - loss: 0.1125 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0626 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 132/200\n",
      "40/40 [==============================] - 0s 156us/step - loss: 0.1116 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0628 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 133/200\n",
      "40/40 [==============================] - 0s 135us/step - loss: 0.1106 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0635 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 134/200\n",
      "40/40 [==============================] - 0s 143us/step - loss: 0.1096 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0638 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 135/200\n",
      "40/40 [==============================] - 0s 125us/step - loss: 0.1086 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0638 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 136/200\n",
      "40/40 [==============================] - 0s 132us/step - loss: 0.1076 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0631 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 137/200\n",
      "40/40 [==============================] - 0s 135us/step - loss: 0.1066 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0620 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 138/200\n",
      "40/40 [==============================] - 0s 127us/step - loss: 0.1057 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0611 - val_acc: 1.0000 - val_mean_pred: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/200\n",
      "40/40 [==============================] - 0s 150us/step - loss: 0.1049 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0606 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 140/200\n",
      "40/40 [==============================] - 0s 144us/step - loss: 0.1041 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0607 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 141/200\n",
      "40/40 [==============================] - 0s 127us/step - loss: 0.1032 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0611 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 142/200\n",
      "40/40 [==============================] - 0s 125us/step - loss: 0.1023 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0617 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 143/200\n",
      "40/40 [==============================] - 0s 148us/step - loss: 0.1013 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0623 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 144/200\n",
      "40/40 [==============================] - 0s 131us/step - loss: 0.1002 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0629 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 145/200\n",
      "40/40 [==============================] - 0s 126us/step - loss: 0.0991 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0633 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 146/200\n",
      "40/40 [==============================] - 0s 140us/step - loss: 0.0980 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0636 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 147/200\n",
      "40/40 [==============================] - 0s 136us/step - loss: 0.0970 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0635 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 148/200\n",
      "40/40 [==============================] - 0s 143us/step - loss: 0.0961 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0628 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 149/200\n",
      "40/40 [==============================] - 0s 134us/step - loss: 0.0952 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0618 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 150/200\n",
      "40/40 [==============================] - 0s 129us/step - loss: 0.0943 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0611 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 151/200\n",
      "40/40 [==============================] - 0s 147us/step - loss: 0.0935 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0610 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 152/200\n",
      "40/40 [==============================] - 0s 136us/step - loss: 0.0926 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0615 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 153/200\n",
      "40/40 [==============================] - 0s 142us/step - loss: 0.0916 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0624 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 154/200\n",
      "40/40 [==============================] - 0s 126us/step - loss: 0.0907 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0630 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 155/200\n",
      "40/40 [==============================] - 0s 133us/step - loss: 0.0897 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0631 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 156/200\n",
      "40/40 [==============================] - 0s 143us/step - loss: 0.0887 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0619 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 157/200\n",
      "40/40 [==============================] - 0s 127us/step - loss: 0.0876 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0603 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 158/200\n",
      "40/40 [==============================] - 0s 130us/step - loss: 0.0867 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0594 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 159/200\n",
      "40/40 [==============================] - 0s 131us/step - loss: 0.0858 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0596 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 160/200\n",
      "40/40 [==============================] - 0s 143us/step - loss: 0.0848 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0605 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 161/200\n",
      "40/40 [==============================] - 0s 140us/step - loss: 0.0839 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0608 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 162/200\n",
      "40/40 [==============================] - 0s 121us/step - loss: 0.0831 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0599 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 163/200\n",
      "40/40 [==============================] - 0s 130us/step - loss: 0.0823 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0585 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 164/200\n",
      "40/40 [==============================] - 0s 147us/step - loss: 0.0815 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0577 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 165/200\n",
      "40/40 [==============================] - 0s 134us/step - loss: 0.0808 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0577 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 166/200\n",
      "40/40 [==============================] - 0s 128us/step - loss: 0.0800 - acc: 0.9500 - mean_pred: 0.5000 - val_loss: 0.0583 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 167/200\n",
      "40/40 [==============================] - 0s 144us/step - loss: 0.0792 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0585 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 168/200\n",
      "40/40 [==============================] - 0s 128us/step - loss: 0.0785 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0579 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 169/200\n",
      "40/40 [==============================] - 0s 139us/step - loss: 0.0778 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0570 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 170/200\n",
      "40/40 [==============================] - 0s 130us/step - loss: 0.0772 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0563 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 171/200\n",
      "40/40 [==============================] - 0s 145us/step - loss: 0.0765 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0561 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 172/200\n",
      "40/40 [==============================] - 0s 146us/step - loss: 0.0759 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0563 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 173/200\n",
      "40/40 [==============================] - 0s 128us/step - loss: 0.0753 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0564 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 174/200\n",
      "40/40 [==============================] - 0s 149us/step - loss: 0.0747 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0560 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 175/200\n",
      "40/40 [==============================] - 0s 127us/step - loss: 0.0741 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0552 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 176/200\n",
      "40/40 [==============================] - 0s 141us/step - loss: 0.0735 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0546 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 177/200\n",
      "40/40 [==============================] - 0s 131us/step - loss: 0.0730 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0543 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 178/200\n",
      "40/40 [==============================] - 0s 133us/step - loss: 0.0724 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0544 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 179/200\n",
      "40/40 [==============================] - 0s 142us/step - loss: 0.0718 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0546 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 180/200\n",
      "40/40 [==============================] - 0s 130us/step - loss: 0.0713 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0544 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 181/200\n",
      "40/40 [==============================] - 0s 134us/step - loss: 0.0708 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0540 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 182/200\n",
      "40/40 [==============================] - 0s 139us/step - loss: 0.0703 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0536 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 183/200\n",
      "40/40 [==============================] - 0s 125us/step - loss: 0.0697 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0534 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 184/200\n",
      "40/40 [==============================] - 0s 136us/step - loss: 0.0692 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0534 - val_acc: 1.0000 - val_mean_pred: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/200\n",
      "40/40 [==============================] - 0s 136us/step - loss: 0.0687 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0534 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 186/200\n",
      "40/40 [==============================] - 0s 118us/step - loss: 0.0682 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0534 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 187/200\n",
      "40/40 [==============================] - 0s 130us/step - loss: 0.0677 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0531 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 188/200\n",
      "40/40 [==============================] - 0s 121us/step - loss: 0.0673 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0527 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 189/200\n",
      "40/40 [==============================] - 0s 129us/step - loss: 0.0668 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0524 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 190/200\n",
      "40/40 [==============================] - 0s 129us/step - loss: 0.0663 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0523 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 191/200\n",
      "40/40 [==============================] - 0s 141us/step - loss: 0.0659 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0522 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 192/200\n",
      "40/40 [==============================] - 0s 137us/step - loss: 0.0654 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0520 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 193/200\n",
      "40/40 [==============================] - 0s 145us/step - loss: 0.0650 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0518 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 194/200\n",
      "40/40 [==============================] - 0s 136us/step - loss: 0.0645 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0516 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 195/200\n",
      "40/40 [==============================] - 0s 135us/step - loss: 0.0641 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0515 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 196/200\n",
      "40/40 [==============================] - 0s 134us/step - loss: 0.0637 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0513 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 197/200\n",
      "40/40 [==============================] - 0s 138us/step - loss: 0.0632 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0512 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 198/200\n",
      "40/40 [==============================] - 0s 126us/step - loss: 0.0628 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0514 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 199/200\n",
      "40/40 [==============================] - 0s 141us/step - loss: 0.0624 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0517 - val_acc: 1.0000 - val_mean_pred: 0.5000\n",
      "Epoch 200/200\n",
      "40/40 [==============================] - 0s 147us/step - loss: 0.0620 - acc: 0.9750 - mean_pred: 0.5000 - val_loss: 0.0517 - val_acc: 1.0000 - val_mean_pred: 0.5000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 200\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "def train(model, X_train, Y_train, X_test, Y_test, batch_size, epochs):\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#               optimizer=keras.optimizers.Adadelta(),\n",
    "             optimizer=keras.optimizers.Adam(\n",
    "                 lr=0.0003,\n",
    "# #                  lr=1e-2,\n",
    "# #                  epsilon=1e-6\n",
    "             ),\n",
    "              metrics=['accuracy', mean_pred])\n",
    "\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              \n",
    "    validation_data=(X_test, Y_test))\n",
    "train(model, X_train, Y_train, X_test, Y_test, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06152468919754028\n",
      "Train accuracy: 0.975\n",
      "Test loss: 0.05168858543038368\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train predictions, truth\n",
      "[0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0\n",
      " 1 1 1]\n",
      "[0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0\n",
      " 1 1 1]\n",
      "test predictions, truth\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0]\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"train predictions, truth\")\n",
    "print(model.predict_classes(X_train))\n",
    "print(np.argmax(Y_train, axis=1))\n",
    "\n",
    "print(\"test predictions, truth\")\n",
    "print(model.predict_classes(X_test))\n",
    "print(np.argmax(Y_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_image(data, idx):\n",
    "    image=X_train[idx, :, :, :]\n",
    "#     print(image.shape)\n",
    "#     img = Image.fromarray(image, 'RGB')\n",
    "#     img.show()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(X_test, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
