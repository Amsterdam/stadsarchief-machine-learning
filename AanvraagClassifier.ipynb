{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aanvraag / besluit classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug commands to see if Tensorflow GPU is supported\n",
    "# from keras import backend as K\n",
    "# print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "import shutil\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.calibration import calibration_curve\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "from examples.aanvraag_besluit.load_data import load_data_aanvraag, preprocess_X\n",
    "from examples.aanvraag_besluit.transformer import Transformer\n",
    "\n",
    "from src.stats import list_stats, show_train_curves, show_prediction_list, show_prediction_images\n",
    "from src.data import split_data\n",
    "from src.image_display import show_image\n",
    "from src import models as own_models\n",
    "\n",
    "# Hot reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tensorflow GPU memory usage to on the fly rather than preallocate.\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_dim = (200, 200, 3);\n",
    "img_dim = (250, 250, 3);\n",
    "# img_dim = (300, 300, 3);\n",
    "# img_dim = (400, 400, 3);\n",
    "\n",
    "[Xtrain_raw, Ytrain_raw, Xvalid_raw, Yvalid_raw] = load_data_aanvraag(\n",
    "    {\n",
    "        'images': f'examples/aanvraag_besluit/eerste_dataset/resized/{img_dim[0]}x{img_dim[1]}/',\n",
    "        'labels': 'examples/aanvraag_besluit/eerste_dataset/labels/'\n",
    "    },\n",
    "    {\n",
    "        'images': f'examples/aanvraag_besluit/tweede_dataset/images/{img_dim[0]}x{img_dim[1]}/',\n",
    "        'labels': 'examples/aanvraag_besluit/tweede_dataset/labels/'\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"shape Xtrain[0]: {Xtrain_raw[0].shape}\")\n",
    "print(f\"shape Xtrain[1]: {Xtrain_raw[1].shape}\")\n",
    "print(f\"shape Ytrain: {Ytrain_raw.shape}\")\n",
    "\n",
    "print(f\"shape Xvalid[0]: {Xvalid_raw[0].shape}\")\n",
    "print(f\"shape Xvalid[1]: {Xvalid_raw[1].shape}\")\n",
    "print(f\"shape Yvalid: {Yvalid_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = Xvalid_raw[1]['reference'] == 'SU10212124_00001.jpg'\n",
    "print(Xvalid_raw[1].loc[ids])\n",
    "Yvalid_raw[ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (encode, transform features and labels)\n",
    "transformer = Transformer()\n",
    "\n",
    "Xdata_mix = Xtrain_raw[1].append(Xvalid_raw[1])\n",
    "transformer.fit(Xdata_mix)\n",
    "\n",
    "Xtrain = preprocess_X(Xtrain_raw[0], Xtrain_raw[1], transformer)\n",
    "Xvalid = preprocess_X(Xvalid_raw[0], Xvalid_raw[1], transformer)\n",
    "# print(Xtrain[1][:4])\n",
    "# print(transformer.decode(Xtrain[1][:4]))\n",
    "print(Xvalid[1][:4])\n",
    "print(transformer.decode(Xvalid[1][:4]))\n",
    "\n",
    "num_features = Xtrain[1].shape[1]\n",
    "print(Xvalid[1].shape)\n",
    "assert Xvalid[1].shape[1] == num_features\n",
    "del Xdata_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain[0].nbytes / 1024**2\n",
    "# Xtrain[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(Ytrain_raw))\n",
    "print(classes)\n",
    "num_classes = 2\n",
    "assert len(classes) == num_classes\n",
    "\n",
    "\n",
    "print('')\n",
    "print('--- TRAIN ---')\n",
    "list_stats(Ytrain_raw)\n",
    "\n",
    "print('')\n",
    "print('--- VALID ---')\n",
    "list_stats(Yvalid_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = preprocessing.LabelEncoder()  # outputs 1d array, binary classification\n",
    "# Ytrain = enc.transform(Ytrain_raw)\n",
    "# Yvalid = enc.transform(Yvalid_raw)\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()  # outputs 2d array, multi class classification\n",
    "assert Ytrain_raw.ndim == 1\n",
    "enc.fit(Ytrain_raw.reshape(-1, 1))\n",
    "\n",
    "print(Ytrain_raw.shape)\n",
    "print(Ytrain_raw[:10])\n",
    "\n",
    "Ytrain = enc.transform(Ytrain_raw.reshape(-1, 1)).toarray()\n",
    "Yvalid = enc.transform(Yvalid_raw.reshape(-1, 1)).toarray()\n",
    "print('Ytrain: ', Ytrain.shape)\n",
    "print('Yvalid: ', Yvalid.shape)\n",
    "print('Ytrain: ', Ytrain[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = own_models.build_multi_feature(num_classes, img_dim, num_features)\n",
    "model = own_models.create_cnn(img_dim, num_classes)\n",
    "# model = own_models.create_cnn_deep(img_dim, num_classes)\n",
    "# model = own_models.create_mlp(num_features, num_classes)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 50\n",
    "\n",
    "run_name = '/cnn_experiment'\n",
    "LOG_DIR = f'./logs{run_name}'\n",
    "shutil.rmtree(LOG_DIR, ignore_errors=True)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        zoom_range=0.1,        # randomly zoom into images\n",
    "        rotation_range=10,      # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1, # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,# randomly shift images vertically (fraction of total height)\n",
    "        shear_range=2.0,  # in degrees\n",
    "        channel_shift_range=0.1,\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False    # randomly flip images\n",
    ")\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(\n",
    "    log_dir=LOG_DIR,\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=True\n",
    ")\n",
    "\n",
    "terminateCB = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "\n",
    "def is_binary(model):\n",
    "    n_classes = model.get_layer('output').output_shape[1]\n",
    "    return n_classes == 1\n",
    "    \n",
    "def compile_model(model):\n",
    "    assert(K.image_data_format() == 'channels_last')\n",
    "    \n",
    "#     if is_binary(model):\n",
    "#         loss= keras.losses.binary_crossentropy\n",
    "#     else:\n",
    "    loss=keras.losses.categorical_crossentropy\n",
    "    \n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "#         optimizer=keras.optimizers.Adadelta(),\n",
    "#         optimizer='rmsprop',\n",
    "#         optimizer='sgd',\n",
    "#         optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "#         optimizer=keras.optimizers.Adam(),        \n",
    "#         optimizer=keras.optimizers.Adam(lr=0.0003),\n",
    "        optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "#         metrics=['accuracy', keras_metrics.recall()]\n",
    "        metrics=['accuracy', keras_metrics.binary_recall(label=0)]\n",
    "\n",
    "    )\n",
    "\n",
    "def train(model, X_train, Y_train, X_test, Y_test, batch_size, epochs):\n",
    "    compile_model(model)\n",
    "    \n",
    "    history = model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              callbacks=[tbCallBack, terminateCB]\n",
    "           )\n",
    "    return history\n",
    "\n",
    "def train_gen(model, X_train, Y_train, X_test, Y_test, batch_size, epochs):\n",
    "    compile_model(model)\n",
    "\n",
    "#     with tf.Session() as s:\n",
    "#         s.run(tf.global_variables_initializer())\n",
    "    history = model.fit_generator(\n",
    "        datagen.flow(X_train,\n",
    "                     Y_train,\n",
    "                     batch_size=batch_size\n",
    "        ),\n",
    "        steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))),\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_test, Y_test),\n",
    "        workers=4,\n",
    "        callbacks=[tbCallBack, terminateCB]\n",
    "    )\n",
    "    return history\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# model = own_models.create_cnn_deep_d(img_dim, num_classes)\n",
    "\n",
    "# Combined data\n",
    "# history = train(model, Xtrain, Ytrain, Xvalid, Yvalid, batch_size, epochs)\n",
    "\n",
    "# Meta data\n",
    "# model = own_models.create_mlp(num_features, num_classes)\n",
    "# history = train(model, Xtrain[1], Ytrain, Xvalid[1], Yvalid, batch_size, epochs)\n",
    "\n",
    "# Img data\n",
    "model = own_models.create_cnn(img_dim, num_classes)\n",
    "# history = train(model, Xtrain[0], Ytrain, Xvalid[0], Yvalid, batch_size, epochs)\n",
    "history = train_gen(model, Xtrain[0], Ytrain, Xvalid[0], Yvalid, batch_size, epochs)\n",
    "\n",
    "show_train_curves(history)\n",
    "\n",
    "difference = time.time() - t0\n",
    "print(f'time: {difference} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model.evaluate(Xtrain[0], Ytrain, verbose=1)\n",
    "print('Train loss:', round(train_score[0], 3))\n",
    "print(f'Train accuracy: {round(train_score[1] * 100, 2)}%')\n",
    "\n",
    "valid_score = model.evaluate(Xvalid[0], Yvalid, verbose=1)\n",
    "print('Test loss:', round(valid_score[0], 3))\n",
    "valid_acc_str = f'{round(valid_score[1] * 100, 2)}%'\n",
    "print(f'Test accuracy: {valid_acc_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"types: {classes}\")\n",
    "\n",
    "print(\"train predictions, truth\")\n",
    "predictions_train =  model.predict(Xtrain[0], verbose=1)\n",
    "show_prediction_list(predictions_train, Ytrain)\n",
    "\n",
    "print(\"test predictions, truth\")\n",
    "predictions_valid = model.predict(Xvalid[0], verbose=1)\n",
    "show_prediction_list(predictions_valid, Yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 11\n",
    "id = Xvalid_raw[1]['reference'][idx]\n",
    "image = Xvalid_raw[0][idx]\n",
    "print(id)\n",
    "show_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Y_train_idx = np.argmax(Y_train, axis=1)        \n",
    "# Y_test_idx = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# print(\"train set:\")\n",
    "# show_prediction_images_new(Xtrain, Ytrain_oh, predictions_train, Ytrain_meta, enc, 10)\n",
    "\n",
    "print(\"test set:\")\n",
    "show_prediction_images(\n",
    "    Xvalid_raw[0],\n",
    "    Yvalid,\n",
    "    predictions_valid,\n",
    "    Xvalid_raw[1]['reference'],\n",
    "    enc,\n",
    "    300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multi_class_to_binary(class_true: np.ndarray, class_pred: np.ndarray):\n",
    "#     # Converting to probablilty that Y_true == 1\n",
    "#     assert class_true.shape[1] == 2  # 2 classes\n",
    "#     assert class_pred.shape[1] == 2  # 2 classes\n",
    "#     assert class_true.shape[0] == class_pred.shape[0]\n",
    "    \n",
    "#     y_true = np.argmax(class_true, axis=1)\n",
    "    \n",
    "# #     pred_ids = np.argmax(class_pred, axis=1)\n",
    "# #     y_prob = class_pred[range(class_pred.shape[0]), pred_ids]\n",
    "#     y_prob = class_pred[:, 1]\n",
    "#     assert y_true.shape == y_prob.shape\n",
    "#     return [y_true, y_prob]\n",
    "    \n",
    "# [y_true, y_prob] = multi_class_to_binary(Y_test, predictions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Yvalid[0:20])\n",
    "# print(np.round(predictions_valid[0:20], 3))\n",
    "# predictions_valid.shape\n",
    "# # print(np.round(y_prob[0:10], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # class_pred = predictions_test\n",
    "# # pred_ids = np.argmax(class_pred, axis=1)\n",
    "# # y_prob = class_pred[range(class_pred.shape[0]), pred_ids]\n",
    "# y_prob = predictions_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reference https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py\n",
    "\n",
    "# n_bins = 5\n",
    "\n",
    "# print(f'accuracy: {test_acc_str}')\n",
    "# conf_avg = np.average(predictions_test)\n",
    "# conf_avg_str = f'{round(conf_avg * 100, 2)}%'\n",
    "# print(f'confidence avg: {conf_avg_str}')\n",
    "\n",
    "# def draw_confidence_histogram(y_prob, n_bins):\n",
    "#     plt.figure()\n",
    "#     plt.title('Confidence histogram')\n",
    "#     plt.xlabel(\"Confidence\")\n",
    "#     plt.ylabel(\"Sample count\")\n",
    "#     plt.hist(y_prob, bins=n_bins)    \n",
    "# draw_confidence_histogram(y_prob, n_bins=n_bins)\n",
    "\n",
    "# def draw_reliability_curve(y_true, y_prob, n_bins):\n",
    "#     plt.figure()\n",
    "#     plt.title('Reliability curve')\n",
    "#     plt.xlabel(\"Confidence\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "#     [prob_true_bins, prob_pred_bins] = calibration_curve(y_true, y_prob, n_bins=n_bins)\n",
    "\n",
    "#     plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "#     plt.plot(prob_pred_bins, prob_true_bins, marker='s')\n",
    "# draw_reliability_curve(y_true, y_prob, n_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "\n",
    "conf_threshold = 0.9\n",
    "labels = ['aanvraag', 'other']\n",
    "\n",
    "# Stats calculation by hand\n",
    "# assert len(labels) == 2\n",
    "# # in binary case confusion matrix is true postive, true negative etc.\n",
    "# tn, fp, fn, tp = confusion.ravel()\n",
    "# # Own recall calculation for sanity check\n",
    "# recall = tp / (tp + fn)\n",
    "# print(f'recall: {recall}')\n",
    "\n",
    "# ROC curve\n",
    "# assert len(labels) == 2  # only works for binary case\n",
    "# scores = np.where(prob[:,0]>conf_threshold, 0,1)\n",
    "# print(y_true[:10, :])\n",
    "# print((predictions_valid[:10, :]).round(2))\n",
    "# metrics.roc_curve(y_true, scores, pos_label=2)\n",
    "\n",
    "#\n",
    "# \n",
    "#\n",
    "def split_bool_arrays(predictions: np.ndarray, threshold, verbose=False):\n",
    "    assert predictions.shape[1] == 2, 'expecting binary prediction in one hot format'\n",
    "    \n",
    "#     y_pred_class = np.argmax(predictions, axis=1)\n",
    "    y_pred_conf = np.amax(predictions, axis=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(y_pred_conf.round(2)[:10])\n",
    "    \n",
    "    certain = y_pred_conf >= threshold\n",
    "    uncertain = np.invert(certain)\n",
    "    return [certain, uncertain]\n",
    "\n",
    "def split_uncertain(predictions: np.ndarray, threshold, elements, verbose=False):\n",
    "    \"\"\"\n",
    "    Split all elements into certain and uncertain buckets\n",
    "    \n",
    "    @return [[elem1_certain, elem1_uncertain], ...]\n",
    "    \"\"\"\n",
    "    for element in elements:\n",
    "        assert element.shape[0] == predictions.shape[0], 'number of an element not equal to number of predictions'\n",
    "    \n",
    "    [certain, uncertain] = split_bool_arrays(predictions, threshold, verbose=verbose)\n",
    "    \n",
    "    results = []\n",
    "    for element in elements:\n",
    "        certain_bucket = element[certain]\n",
    "        uncertain_bucket = element[uncertain]\n",
    "        results.append([certain_bucket, uncertain_bucket])\n",
    "    return results\n",
    "    \n",
    "results = split_uncertain(predictions_valid, conf_threshold, [Xvalid_raw[0], Xvalid_raw[1], Yvalid, predictions_valid])\n",
    "print('image certain shape: ', results[0][0].shape)\n",
    "print('image uncertain shape: ', results[0][1].shape)\n",
    "print('meta certain shape: ', results[1][0].shape)\n",
    "print('meta uncertain shape: ', results[1][1].shape)\n",
    "\n",
    "[\n",
    "    _, # img\n",
    "    _, # meta\n",
    "    Yvalid_buckets, # true\n",
    "    Yvalid_pred_buckets, # prediction\n",
    "] = results\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "def create_reports(y_true_oh: np.ndarray, y_pred_oh: np.ndarray):\n",
    "    assert y_true_oh.shape == y_pred_oh.shape\n",
    "    assert y_true_oh.shape[1] == 2, 'expecting binary one hot inputs'\n",
    "    y_true = enc.inverse_transform(y_true_oh)\n",
    "    y_pred = enc.inverse_transform(y_pred_oh)\n",
    "    \n",
    "#     confusion = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     plt.imshow(confusion, cmap='binary', interpolation='None')\n",
    "#     plt.show()\n",
    "#     print(f'confusion matrix:\\n{confusion}')\n",
    "    y_true_pd = pd.Series(y_true.ravel())\n",
    "    y_pred_pd = pd.Series(y_pred.ravel())\n",
    "    crosstab = pd.crosstab(y_true_pd, y_pred_pd, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, target_names=labels)\n",
    "\n",
    "    return [crosstab, report]\n",
    "\n",
    "\n",
    "def show_reports(y_true_oh: np.ndarray, y_pred_oh: np.ndarray):\n",
    "    [crosstab, report] = create_reports(y_true_oh, y_pred_oh)\n",
    "    print(crosstab)\n",
    "    \n",
    "    print()\n",
    "    print(report)\n",
    "    \n",
    "\n",
    "print()\n",
    "print('--- certain bucket stats ---')\n",
    "show_reports(Yvalid_buckets[0], Yvalid_pred_buckets[0])\n",
    "\n",
    "print()\n",
    "print('--- uncertain bucket stats ---')\n",
    "show_reports(Yvalid_buckets[1], Yvalid_pred_buckets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "\n",
    "predictions = predictions_valid\n",
    "\n",
    "def show_results(threshold):\n",
    "    total = predictions.shape[0]\n",
    "    [certain, uncertain] = split_bool_arrays(predictions, threshold)\n",
    "    certain_count = np.sum(certain)\n",
    "    uncertain_count = np.sum(uncertain)\n",
    "#     print(certain[:10])\n",
    "#     print(uncertain[:10])\n",
    "    \n",
    "    # Show counts of split\n",
    "    certain_percentage = certain_count/total*100\n",
    "    uncertain_percentage = uncertain_count/total*100\n",
    "    counts_df = pd.DataFrame([\n",
    "        [certain_count, uncertain_count, total],\n",
    "        [certain_percentage, uncertain_percentage, 100.0]\n",
    "    ],\n",
    "                      columns=['certain', 'uncertain', 'total'],\n",
    "                      index=['aboslute', 'relative'])\n",
    "    \n",
    "    # Show metrics of splits\n",
    "    [\n",
    "        Ytrue_buckets,\n",
    "        Ypred_buckets,\n",
    "    ] = split_uncertain(predictions, threshold, [Yvalid, predictions_valid])\n",
    "    \n",
    "    \n",
    "    certain_not_empty = Ytrue_buckets[0].shape[0] > 0\n",
    "    if certain_not_empty:\n",
    "        y_true = enc.inverse_transform(Ytrue_buckets[0])\n",
    "        y_pred = enc.inverse_transform(Ypred_buckets[0])\n",
    "        certain_recall = recall_score(y_true, y_pred, pos_label='aanvraag')    \n",
    "        print(f'certain examples:\\t\\t{color.BOLD}{round(certain_percentage, 1)}%{color.END}', end='')\n",
    "        print(f'\\t-> recall: {color.BOLD}{round(certain_recall*100, 2)}%{color.END}')\n",
    "    else:\n",
    "        print(f'certain examples:\\t\\t{color.BOLD}{round(certain_percentage, 1)}%{color.END}')\n",
    "    print(f'uncertain examples:\\t\\t{color.BOLD}{round(uncertain_percentage, 1)}%{color.END}')\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(counts_df.round(2))\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(f'{color.BOLD}## Certain examples{color.END}')\n",
    "    if certain_not_empty:\n",
    "        show_reports(Ytrue_buckets[0], Ypred_buckets[0])\n",
    "    else:\n",
    "        print('no data')\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print(f'{color.BOLD}## Uncertain examples{color.END}')\n",
    "    if Ytrue_buckets[1].shape[0] == 0:\n",
    "        print('no data')\n",
    "    else:\n",
    "        show_reports(Ytrue_buckets[1], Ypred_buckets[1])\n",
    "\n",
    "widget = widgets.FloatSlider(\n",
    "    value=0.9,\n",
    "    min=0.5,\n",
    "    max=1.0,\n",
    "    step=0.005,\n",
    "    continuous_update=False,\n",
    "    description='Threshold:',\n",
    "    readout=True,\n",
    "    readout_format='.3f',\n",
    ")\n",
    "\n",
    "interact(show_results, threshold=widget);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_overview(Y_oh, pred_oh, references, encoder):\n",
    "    Y_class = encoder.inverse_transform(Y_oh)\n",
    "    pred_class = encoder.inverse_transform(pred_oh)\n",
    "    \n",
    "    data = {'reference': references, 'label': Y_class[:, 0], 'prediction': pred_class[:, 0]}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "DIR = './output'\n",
    "os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "df = predictions_overview(Ytrain, predictions_train, Xtrain_raw[1]['reference'], enc)\n",
    "df.to_csv(os.path.join(DIR, 'train_predictions.csv'))\n",
    "print('---TRAIN---')\n",
    "print(df)\n",
    "\n",
    "df = predictions_overview(Yvalid, predictions_valid, Xvalid_raw[1]['reference'], enc)\n",
    "df.to_csv(os.path.join(DIR, 'validation_predictions.csv'))\n",
    "print('---VALID---')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0 + 2.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
