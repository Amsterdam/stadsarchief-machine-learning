{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aanvraag / besluit classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras import layers\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "\n",
    "from stats import list_stats, show_train_curves, show_prediction_list, show_prediction_images_new\n",
    "# from stats import show_train_curves\n",
    "from data import split_data\n",
    "from examples.aanvraag_besluit.load_data import load_data_aanvraag, preprocess_X\n",
    "from examples.aanvraag_besluit.transformer import Transformer\n",
    "from image_display import show_image\n",
    "\n",
    "# Hot reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_dim = (200, 200, 3);\n",
    "img_dim = (250, 250, 3);\n",
    "# img_dim = (300, 300, 3);\n",
    "# img_dim = (400, 400, 3);\n",
    "\n",
    "[Xtrain_raw, Ytrain_raw, Xvalid_raw, Yvalid_raw] = load_data_aanvraag(\n",
    "    {\n",
    "        'images': f'examples/aanvraag_besluit/eerste_dataset/resized/{img_dim[0]}x{img_dim[1]}/',\n",
    "        'labels': 'examples/aanvraag_besluit/eerste_dataset/labels/'\n",
    "    },\n",
    "    {\n",
    "        'images': f'examples/aanvraag_besluit/tweede_dataset/images/{img_dim[0]}x{img_dim[1]}/',\n",
    "        'labels': 'examples/aanvraag_besluit/tweede_dataset/labels/'\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"shape Xtrain[0]: {Xtrain_raw[0].shape}\")\n",
    "print(f\"shape Xtrain[1]: {Xtrain_raw[1].shape}\")\n",
    "print(f\"shape Ytrain: {Ytrain_raw.shape}\")\n",
    "\n",
    "print(f\"shape Xvalid[0]: {Xvalid_raw[0].shape}\")\n",
    "print(f\"shape Xvalid[1]: {Xvalid_raw[1].shape}\")\n",
    "print(f\"shape Yvalid: {Yvalid_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (encode, transform features and labels)\n",
    "transformer = Transformer()\n",
    "\n",
    "Xdata_mix = Xtrain_raw[1].append(Xvalid_raw[1])\n",
    "transformer.fit(Xdata_mix)\n",
    "\n",
    "Xtrain = preprocess_X(Xtrain_raw[0], Xtrain_raw[1], transformer)\n",
    "Xvalid = preprocess_X(Xvalid_raw[0], Xvalid_raw[1], transformer)\n",
    "# print(Xtrain[1][:4])\n",
    "# print(transformer.decode(Xtrain[1][:4]))\n",
    "print(Xvalid[1][:4])\n",
    "print(transformer.decode(Xvalid[1][:4]))\n",
    "\n",
    "num_features = Xtrain[1].shape[1]\n",
    "print(Xvalid[1].shape)\n",
    "assert Xvalid[1].shape[1] == num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(Ytrain_raw))\n",
    "print(classes)\n",
    "num_classes = 2\n",
    "assert len(classes) == num_classes\n",
    "\n",
    "\n",
    "print('')\n",
    "print('--- TRAIN ---')\n",
    "list_stats(Ytrain_raw)\n",
    "\n",
    "print('')\n",
    "print('--- VALID ---')\n",
    "list_stats(Yvalid_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = preprocessing.LabelEncoder()  # outputs 1d array, binary classification\n",
    "# Ytrain = enc.transform(Ytrain_raw)\n",
    "# Yvalid = enc.transform(Yvalid_raw)\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()  # outputs 2d array, multi class classification\n",
    "assert Ytrain_raw.ndim == 1\n",
    "enc.fit(Ytrain_raw.reshape(-1, 1))\n",
    "\n",
    "print(Ytrain_raw.shape)\n",
    "print(Ytrain_raw[:10])\n",
    "\n",
    "Ytrain = enc.transform(Ytrain_raw.reshape(-1, 1)).toarray()\n",
    "Yvalid = enc.transform(Yvalid_raw.reshape(-1, 1)).toarray()\n",
    "print('Ytrain: ', Ytrain.shape)\n",
    "print('Yvalid: ', Yvalid.shape)\n",
    "print('Ytrain: ', Ytrain[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_alpha=0.1\n",
    "drop_chance=0.1\n",
    "\n",
    "def create_mlp(num_features):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=num_features, name=\"input-mlp\"))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=lr_alpha))\n",
    "    model.add(Dropout(drop_chance))\n",
    "    \n",
    "    model.add(Dense(8))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=lr_alpha))\n",
    "    model.add(Dropout(drop_chance))\n",
    "    \n",
    "#     model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_cnn(img_dim):\n",
    "    inputs = Input(shape=img_dim, name=\"input-cnn\")\n",
    "\n",
    "    x = Conv2D(16, (3, 3), use_bias=False)(inputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=lr_alpha)(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), use_bias=False)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=lr_alpha)(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), use_bias=False)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=lr_alpha)(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), use_bias=False)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=lr_alpha)(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(16, kernel_initializer = RandomUniform())(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=lr_alpha)(x)\n",
    "    x = Dropout(drop_chance)(x)\n",
    "    \n",
    "#     x = Dense(2, activation=\"softmax\", name='output')(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_multi_feature(num_classes, img_dim, num_features):\n",
    "    # Multi feature system based on https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
    "    print('img_dim: ', img_dim)\n",
    "    print('num_features: ', num_features)\n",
    "    \n",
    "    # create the MLP and CNN models\n",
    "    mlp = create_mlp(num_features)\n",
    "    cnn = create_cnn(img_dim)\n",
    "\n",
    "    # Merge the two branches\n",
    "    combinedInput = layers.concatenate([mlp.output, cnn.output])\n",
    "\n",
    "    # Final logic\n",
    "    x = Dense(8, activation=\"relu\")(combinedInput)\n",
    "    x = Dropout(drop_chance)(x)\n",
    "    \n",
    "    x = Dense(num_classes, activation=\"softmax\", name='output')(x)\n",
    "\n",
    "    # our final model will accept categorical/numerical data on the MLP\n",
    "    # input and images on the CNN input\n",
    "    # Output: softmax\n",
    "    model = Model(inputs=[cnn.input, mlp.input], outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_multi_feature(num_classes, img_dim, num_features)\n",
    "# model = create_cnn(img_dim)\n",
    "# model = create_mlpB(num_features)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain = [0, Ytrain]\n",
    "# Xvalid = [0, Yvalid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        zoom_range=0.1,        # randomly zoom into images\n",
    "        rotation_range=5,      # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1, # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,# randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False    # randomly flip images\n",
    ")\n",
    " \n",
    "\n",
    "def is_binary(model):\n",
    "    n_classes = model.get_layer('output').output_shape[1]\n",
    "    return n_classes == 1\n",
    "    \n",
    "def compile_model(model):\n",
    "    assert(K.image_data_format() == 'channels_last')\n",
    "    \n",
    "#     if is_binary(model):\n",
    "#         loss= keras.losses.binary_crossentropy\n",
    "#     else:\n",
    "    loss=keras.losses.categorical_crossentropy\n",
    "    \n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "#         optimizer=keras.optimizers.Adadelta(),\n",
    "#         optimizer='rmsprop',\n",
    "        optimizer=keras.optimizers.Adam(),        \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "def train(model, X_train, Y_train, X_test, Y_test, batch_size, epochs):\n",
    "    compile_model(model)\n",
    "    \n",
    "    history = model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(X_test, Y_test)\n",
    "           )\n",
    "    return history\n",
    "\n",
    "def train_gen(model, X_train, Y_train, X_test, Y_test, batch_size, epochs):\n",
    "    compile_model(model)\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "        datagen.flow(X_train,\n",
    "                     Y_train,\n",
    "                     batch_size=batch_size\n",
    "        ),\n",
    "        steps_per_epoch=int(np.ceil(X_train.shape[0] / float(batch_size))),\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_test, Y_test),\n",
    "        workers=4\n",
    "    )\n",
    "    return history\n",
    "history = train(model, Xtrain, Ytrain, Xvalid, Yvalid, batch_size, epochs)\n",
    "# history = train(model, Xtrain[1], Ytrain, Xvalid[1], Yvalid, batch_size, epochs)\n",
    "# history = train_gen(model, Xtrain[0], Ytrain, Xvalid[0], Yvalid, batch_size, epochs)\n",
    "\n",
    "show_train_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train = np.argmax(Y_train, axis=1)\n",
    "# Y_test = np.argmax(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model.evaluate(Xtrain, Ytrain, verbose=1)\n",
    "print('Train loss:', round(train_score[0], 3))\n",
    "print(f'Train accuracy: {round(train_score[1] * 100, 2)}%')\n",
    "\n",
    "valid_score = model.evaluate(Xvalid, Yvalid, verbose=1)\n",
    "print('Test loss:', round(valid_score[0], 3))\n",
    "valid_acc_str = f'{round(valid_score[1] * 100, 2)}%'\n",
    "print(f'Test accuracy: {valid_acc_str}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"types: {classes}\")\n",
    "\n",
    "print(\"train predictions, truth\")\n",
    "predictions_train =  model.predict(Xtrain, verbose=1)\n",
    "show_prediction_list(predictions_train, Ytrain_oh)\n",
    "\n",
    "print(\"test predictions, truth\")\n",
    "predictions_valid = model.predict(Xvalid, verbose=1)\n",
    "show_prediction_list(predictions_valid, Yvalid_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 11\n",
    "print(ids_test[idx])\n",
    "show_image(Xvalid, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Y_train_idx = np.argmax(Y_train, axis=1)        \n",
    "# Y_test_idx = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# print(\"train set:\")\n",
    "# show_prediction_images_new(Xtrain, Ytrain_oh, predictions_train, Ytrain_meta, enc, 10)\n",
    "\n",
    "print(\"test set:\")\n",
    "show_prediction_images_new(Xvalid, Yvalid_oh, predictions_valid, Yvalid_meta, enc, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multi_class_to_binary(class_true: np.ndarray, class_pred: np.ndarray):\n",
    "#     # Converting to probablilty that Y_true == 1\n",
    "#     assert class_true.shape[1] == 2  # 2 classes\n",
    "#     assert class_pred.shape[1] == 2  # 2 classes\n",
    "#     assert class_true.shape[0] == class_pred.shape[0]\n",
    "    \n",
    "#     y_true = np.argmax(class_true, axis=1)\n",
    "    \n",
    "# #     pred_ids = np.argmax(class_pred, axis=1)\n",
    "# #     y_prob = class_pred[range(class_pred.shape[0]), pred_ids]\n",
    "#     y_prob = class_pred[:, 1]\n",
    "#     assert y_true.shape == y_prob.shape\n",
    "#     return [y_true, y_prob]\n",
    "    \n",
    "# [y_true, y_prob] = multi_class_to_binary(Y_test, predictions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Yvalid[0:20])\n",
    "print(np.round(predictions_valid[0:20], 3))\n",
    "predictions_valid.shape\n",
    "# print(np.round(y_prob[0:10], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_pred = predictions_test\n",
    "# pred_ids = np.argmax(class_pred, axis=1)\n",
    "# y_prob = class_pred[range(class_pred.shape[0]), pred_ids]\n",
    "y_prob = predictions_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py\n",
    "\n",
    "n_bins = 5\n",
    "\n",
    "print(f'accuracy: {test_acc_str}')\n",
    "conf_avg = np.average(predictions_test)\n",
    "conf_avg_str = f'{round(conf_avg * 100, 2)}%'\n",
    "print(f'confidence avg: {conf_avg_str}')\n",
    "\n",
    "def draw_confidence_histogram(y_prob, n_bins):\n",
    "    plt.figure()\n",
    "    plt.title('Confidence histogram')\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Sample count\")\n",
    "    plt.hist(y_prob, bins=n_bins)    \n",
    "draw_confidence_histogram(y_prob, n_bins=n_bins)\n",
    "\n",
    "def draw_reliability_curve(y_true, y_prob, n_bins):\n",
    "    plt.figure()\n",
    "    plt.title('Reliability curve')\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    [prob_true_bins, prob_pred_bins] = calibration_curve(y_true, y_prob, n_bins=n_bins)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    plt.plot(prob_pred_bins, prob_true_bins, marker='s')\n",
    "draw_reliability_curve(y_true, y_prob, n_bins)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
